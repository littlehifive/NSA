---
title: "Measurement analysis"
author: "Michael Wu"
date: "2022-10-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r data preparation, include = F}
library(tidyverse)
library(targets)

library(lavaan)
library(semTools)
library(kfa)
#library(mrautomatr)

# using the master dataset with 1-5 (or 1-4) coding
#dat <- tar_read(dat_all_cleaned_reverse_coded)
dat <- tar_read(dat_all_cleaned_combine_coded)
# not using the combine coded master dataset (i.e., 1-3)
# dat_t <- tar_read(dat_all_cleaned_combine_coded)

dim(dat)
```

# Item-level correlation

```{r item-level correlation}
# correlation matrix
# baseline
psych::cor.ci(dat |> select(capable_person_b:pressure_parent_teacher_b))

# endline
psych::cor.ci(dat |> select(capable_person_e:pressure_parent_teacher_e))
```

# EFA

## Baseline EFA

The following items (negatively worded) do not correlate with the other items in their original scales, and substantially lowered reliability if they are not reverse coded when calculating alpha for the original scales. Therefore, I dropped the following items from factor analysis at both baseline and endline (`teacher_like_me_b, concerned_abt_impression_b, feel_outsider_b, ppl_like_me_b`).

Because the recommended sample size for each fold is n = 200 in k-fold cross-validation, k is essentially 2 for this dataset.

Based on 2-fold cross-validation, a 3-factor structure has the highest average reliability across folds. There is also no standardized loading across folds under 0.3. This factor structure also reasonably reflects three distinct constructs: f1 - self-esteem and social evaluative threat, f2 - stereotype threat, f3 - academic threat.

`f1 =~ capable_person_b + confident_abt_future_b + comfortable_who_i_am_b + feel_smart_b + respect_lookup_b + belong_school_b + people_accept_b + comfortable_at_school_b`

`f2 =~ worried_other_think_b + worry_abt_dumb_b + nervous_worried_b + worry_ppl_dislike_b + conclusion_other_deaf_b + conclusion_my_perform_b + conclusion_abt_me_b`

`f3 =~ bad_grades_b + not_understand_class_b + not_understand_homework_b + bad_class_teacher_b + trouble_studying_b + pressure_parent_teacher_b`

```{r Baseline EFA}
# Scree plot
dat_kfa_b <- dat |> select(capable_person_b:pressure_parent_teacher_b,
                           -c(teacher_like_me_b, concerned_abt_impression_b,
                           feel_outsider_b, ppl_like_me_b))

efa_b <- psych::fa(dat_kfa_b, nfactors = 6)

plot.scree(efa_b)


# k is essentially 2 because the recommended sample size for each fold is 200
# (Curran, Bollen, Chen, Paxton, & Kirby, 2003)
kfa_b <- kfa(dat_kfa_b,
             k = NULL,
             m = 6,
             seed = 1234)

kfa_report(models = kfa_b,
           file.name = "analysis/kfa_report_baseline",
           report.title = "K-fold Factor Analysis - Baseline",
           report.format = "html_document")

```

## Endline EFA

The same 3-factor model at endline also yields the best CFA fit and reliability across folds. Reliability is lower as compared to baseline but still in an acceptable range.

```{r Endline EFA}
# Scree plot
dat_kfa_e <- dat |> select(capable_person_e:pressure_parent_teacher_e,
                           -c(teacher_like_me_e, concerned_abt_impression_e,
                           feel_outsider_e, ppl_like_me_e))

efa_e <- psych::fa(dat_kfa_e, nfactors = 6)

plot.scree(efa_e)


# k is essentially 2 because the recommended sample size for each fold is 200
# (Curran, Bollen, Chen, Paxton, & Kirby, 2003)
kfa_e <- kfa(dat_kfa_e,
             k = NULL,
             m = 6,
             seed = 1234)

kfa_report(models = kfa_e,
           file.name = "analysis/kfa_report_endline",
           report.title = "K-fold Factor Analysis - Endline",
           report.format = "html_document")

```

# CFA

We then run a CFA model at both baseline and endline based on the suggested solutions from kfa.

In all models, model fits are evaluated using Hu and Bentler's (1999) criteria: RMSEA (Root Mean Square Error Of Approximation) \< 0.06, CFI (Comparative Fit Index) \< 0.95, TLI (Tucker--Lewis Index) \< 0.95, SRMR (Standardized Root Mean Squared Residual) \< 0.08.

## Baseline CFA (combined)

```{r Baseline CFA combined}
# variable names
var_b <- c("capable_person_b" , "confident_abt_future_b" , "comfortable_who_i_am_b" , "feel_smart_b" , "respect_lookup_b" , "belong_school_b" , "people_accept_b" , "comfortable_at_school_b",

   "worried_other_think_b" , "worry_abt_dumb_b" , "nervous_worried_b" , "worry_ppl_dislike_b" , "conclusion_other_deaf_b" , "conclusion_my_perform_b" , "conclusion_abt_me_b",

   "bad_grades_b" , "not_understand_class_b" , "not_understand_homework_b" , "bad_class_teacher_b" , "trouble_studying_b" , "pressure_parent_teacher_b") 

# cfa model
model_cfa_b <- '
  # latent variable
  
  f1 =~ capable_person_b + confident_abt_future_b + comfortable_who_i_am_b + feel_smart_b + respect_lookup_b + belong_school_b + people_accept_b + comfortable_at_school_b
  
  f2 =~ worried_other_think_b + worry_abt_dumb_b + nervous_worried_b + worry_ppl_dislike_b + conclusion_other_deaf_b + conclusion_my_perform_b + conclusion_abt_me_b
  
  f3 =~ bad_grades_b + not_understand_class_b + not_understand_homework_b + bad_class_teacher_b + trouble_studying_b + pressure_parent_teacher_b

'

# only one 1 in this variable, collapsing that to 2
dat_temp <- dat |> mutate(people_accept_b = recode(people_accept_b, `1` = 2))

# cfa with ordinally scaled items
fit_cfa_b <- lavaan::cfa(model_cfa_b,
                         data = dat_temp,
                         ordered = var_b,
                         cluster = "class_id")

summary(fit_cfa_b, fit.measures = TRUE, standardized = TRUE)

fit_measures_cfa_b <- fitMeasures(fit_cfa_b)
```

The fit indices in the initial model are not great (CFI = `r fit_measures_cfa_b["cfi.scaled"]`, TLI = `r fit_measures_cfa_b["tli.scaled"]`, RMSEA (`r fit_measures_cfa_b["rmsea.scaled"]`, SRMR = `r fit_measures_cfa_b["srmr.scaled"]`).

Therefore, I check the modification indices (MI): An MI is an estimate of the amount by which the chi-square would be reduced if a single parameter restriction were to be removed from the model. If a parameter is added based on a large MI, this is called a "post hoc model modification" and represents a data-driven modification of the original hypothesized model. It is not uncommon in practice for researchers to consult MIs to suggest model modifications that lead to a "better" fitting model. <https://centerstat.org/what-are-modification-indices-and-should-i-use-them-when-fitting-sems-to-my-own-data/>

I am only adding in residual covariance links to allow the two observed variables to be correlated. This is sometimes done if it is believed that the two variables have something in common that is not captured by the latent variables. The added links should make theoretical sense. This way of increasing model fit is also justified here because I collapsed 6 scales into 3. The self-related items and the belonging-related items fall into the same factor from EFA, but certain items may be related to each other that are not captured by the latent factor. The same goes to the stereotype and stereotype-threat related items.

```{r Baseline CFA combined updated}
# only adding in residual covariance
# > 10
# test the ones over 30

modindices(fit_cfa_b) |> filter(mi > 3.84 & op == "~~") |> arrange(desc(mi))

# cfa model

model_cfa_b <- '
  # latent variable
  
  f1 =~ capable_person_b + confident_abt_future_b + comfortable_who_i_am_b + feel_smart_b + respect_lookup_b + belong_school_b + people_accept_b + comfortable_at_school_b
  
  f2 =~ worried_other_think_b + worry_abt_dumb_b + nervous_worried_b + worry_ppl_dislike_b + conclusion_other_deaf_b + conclusion_my_perform_b + conclusion_abt_me_b
  
  f3 =~ bad_grades_b + not_understand_class_b + not_understand_homework_b + bad_class_teacher_b + trouble_studying_b + pressure_parent_teacher_b

  # residual covariance

conclusion_other_deaf_b ~~ conclusion_my_perform_b
confident_abt_future_b ~~ comfortable_who_i_am_b
comfortable_who_i_am_b ~~ comfortable_at_school_b

worry_abt_dumb_b ~~ worried_other_think_b
worry_abt_dumb_b ~~ conclusion_abt_me_b
worry_abt_dumb_b ~~ conclusion_my_perform_b
worry_abt_dumb_b ~~ conclusion_other_deaf_b
worry_abt_dumb_b ~~ respect_lookup_b
'

# only one 1 in this variable, collapsing that to 2
dat_temp <- dat |> mutate(people_accept_b = recode(people_accept_b, `1` = 2))

# cfa with ordinally scaled items
fit_cfa_b <- lavaan::cfa(model_cfa_b,
                         data = dat_temp,
                         ordered = var_b,
                         cluster = "class_id")

summary(fit_cfa_b, fit.measures = TRUE, standardized = TRUE)

fit_measures_cfa_b <- fitMeasures(fit_cfa_b)
```

After adding in the additional links, the fit indices are acceptable(CFI, TLI) to good (RMSEA, SRMR).

## Baseline CFA (separate)

```{r CFA separate three factors}
var_f1_b <- c("capable_person_b" , "confident_abt_future_b" , "comfortable_who_i_am_b" , "feel_smart_b" , "respect_lookup_b" , "belong_school_b" , "people_accept_b" , "comfortable_at_school_b") 

var_f2_b <- c("worried_other_think_b" , "worry_abt_dumb_b" , "nervous_worried_b" , "worry_ppl_dislike_b" , "conclusion_other_deaf_b" , "conclusion_my_perform_b" , "conclusion_abt_me_b") 

var_f3_b <- c("bad_grades_b" , "not_understand_class_b" , "not_understand_homework_b" , "bad_class_teacher_b" , "trouble_studying_b" , "pressure_parent_teacher_b") 

# cfa model
model_cfa_f1_b <- '
  f1 =~ capable_person_b + confident_abt_future_b + comfortable_who_i_am_b + feel_smart_b + respect_lookup_b + belong_school_b + people_accept_b + comfortable_at_school_b

respect_lookup_b ~~         people_accept_b
comfortable_who_i_am_b ~~ comfortable_at_school_b
'

model_cfa_f2_b <- '
  f2 =~ worried_other_think_b + worry_abt_dumb_b + nervous_worried_b + worry_ppl_dislike_b + conclusion_other_deaf_b + conclusion_my_perform_b + conclusion_abt_me_b

worried_other_think_b ~~        worry_abt_dumb_b
conclusion_other_deaf_b ~~ conclusion_my_perform_b
'

model_cfa_f3_b <- '
  f3 =~ bad_grades_b + not_understand_class_b + not_understand_homework_b + bad_class_teacher_b + trouble_studying_b + pressure_parent_teacher_b
'

# cfa with ordinally scaled items
fit_cfa_f1_b <- lavaan::cfa(model_cfa_f1_b,
                         data = dat,
                         ordered = var_f1_b,
                         cluster = "class_id")

summary(fit_cfa_f1_b, fit.measures = TRUE, standardized = TRUE)

fit_cfa_f2_b <- lavaan::cfa(model_cfa_f2_b,
                         data = dat,
                         ordered = var_f2_b,
                         cluster = "class_id")

summary(fit_cfa_f2_b, fit.measures = TRUE, standardized = TRUE)

fit_cfa_f3_b <- lavaan::cfa(model_cfa_f3_b,
                         data = dat,
                         ordered = var_f3_b,
                         cluster = "class_id")

summary(fit_cfa_f3_b, fit.measures = TRUE, standardized = TRUE)
```

The fit is good for the CFA model for each factor.

## Endline CFA combined

```{r Endline CFA combined}
# variable name
var_e <- c("capable_person_e" , "confident_abt_future_e" , "comfortable_who_i_am_e" , "feel_smart_e" , "respect_lookup_e" , "belong_school_e" , "people_accept_e" , "comfortable_at_school_e",

   "worried_other_think_e" , "worry_abt_dumb_e" , "nervous_worried_e" , "worry_ppl_dislike_e" , "conclusion_other_deaf_e" , "conclusion_my_perform_e" , "conclusion_abt_me_e",

   "bad_grades_e" , "not_understand_class_e" , "not_understand_homework_e" , "bad_class_teacher_e" , "trouble_studying_e" , "pressure_parent_teacher_e") 

# cfa model
model_cfa_e <- '
  # latent variable
  
  f1 =~ capable_person_e + confident_abt_future_e + comfortable_who_i_am_e + feel_smart_e + respect_lookup_e + belong_school_e + people_accept_e + comfortable_at_school_e
  
  f2 =~ worried_other_think_e + worry_abt_dumb_e + nervous_worried_e + worry_ppl_dislike_e + conclusion_other_deaf_e + conclusion_my_perform_e + conclusion_abt_me_e
  
  f3 =~ bad_grades_e + not_understand_class_e + not_understand_homework_e + bad_class_teacher_e + trouble_studying_e + pressure_parent_teacher_e

'

# cfa with ordinally scaled items
fit_cfa_e <- lavaan::cfa(model_cfa_e,
                         data = dat,
                         ordered = var_e,
                         cluster = "class_id")

summary(fit_cfa_e, fit.measures = TRUE, standardized = TRUE)

fit_measures_cfa_e <- fitMeasures(fit_cfa_e)
```

The fit indices in the initial model are not great (CFI = `r fit_measures_cfa_e["cfi.scaled"]`, TLI = `r fit_measures_cfa_e["tli.scaled"]`, RMSEA (`r fit_measures_cfa_e["rmsea.scaled"]`, SRMR = `r fit_measures_cfa_e["srmr.scaled"]`).

Again, I check the modification indices (MI).

```{r Endline CFA combined updated}
# threat to generalizability
# clustered standard error
# tradeoff between measurement error vs. reflecting clustering -> internal validity

# only adding in residual covariance
modindices(fit_cfa_e) |> filter(mi > 10 & op == "~~") |> arrange(desc(mi))

# cfa model
model_cfa_e <- '
  # latent variable
  
  f1 =~ capable_person_e + confident_abt_future_e + comfortable_who_i_am_e + feel_smart_e + respect_lookup_e + belong_school_e + people_accept_e + comfortable_at_school_e
  
  f2 =~ worried_other_think_e + worry_abt_dumb_e + nervous_worried_e + worry_ppl_dislike_e + conclusion_other_deaf_e + conclusion_my_perform_e + conclusion_abt_me_e
  
  f3 =~ bad_grades_e + not_understand_class_e + not_understand_homework_e + bad_class_teacher_e + trouble_studying_e + pressure_parent_teacher_e

  # residual covariance

conclusion_other_deaf_e ~~   conclusion_my_perform_e
confident_abt_future_e ~~    comfortable_who_i_am_e
comfortable_who_i_am_e ~~   comfortable_at_school_e

worry_abt_dumb_e ~~ worried_other_think_e
worry_abt_dumb_e ~~       conclusion_abt_me_e
worry_abt_dumb_e ~~   conclusion_my_perform_e
worry_abt_dumb_e ~~   conclusion_other_deaf_e
worry_abt_dumb_e ~~ respect_lookup_e
'

# cfa with ordinally scaled items
fit_cfa_e <- lavaan::cfa(model_cfa_e,
                         data = dat,
                         ordered = var_e,
                         cluster = "class_id")

summary(fit_cfa_e, fit.measures = TRUE, standardized = TRUE)

fit_measures_cfa_e <- fitMeasures(fit_cfa_e)
```

After adding in the additional links, the fit indices are acceptable(CFI, TLI) to good (RMSEA, SRMR).

## Endline CFA (separate)

```{r}
var_f1_e <- c("capable_person_e" , "confident_abt_future_e" , "comfortable_who_i_am_e" , "feel_smart_e" , "respect_lookup_e" , "belong_school_e" , "people_accept_e" , "comfortable_at_school_e") 

var_f2_e <- c("worried_other_think_e" , "worry_abt_dumb_e" , "nervous_worried_e" , "worry_ppl_dislike_e" , "conclusion_other_deaf_e" , "conclusion_my_perform_e" , "conclusion_abt_me_e") 

var_f3_e <- c("bad_grades_e" , "not_understand_class_e" , "not_understand_homework_e" , "bad_class_teacher_e" , "trouble_studying_e" , "pressure_parent_teacher_e") 

# cfa model
model_cfa_f1_e <- '
  f1 =~ capable_person_e + confident_abt_future_e + comfortable_who_i_am_e + feel_smart_e + respect_lookup_e + belong_school_e + people_accept_e + comfortable_at_school_e

respect_lookup_e ~~         people_accept_e
comfortable_who_i_am_e ~~ comfortable_at_school_e
'

model_cfa_f2_e <- '
  f2 =~ worried_other_think_e + worry_abt_dumb_e + nervous_worried_e + worry_ppl_dislike_e + conclusion_other_deaf_e + conclusion_my_perform_e + conclusion_abt_me_e

worried_other_think_e ~~        worry_abt_dumb_e
conclusion_other_deaf_e ~~ conclusion_my_perform_e
'

model_cfa_f3_e <- '
  f3 =~ bad_grades_e + not_understand_class_e + not_understand_homework_e + bad_class_teacher_e + trouble_studying_e + pressure_parent_teacher_e
'

# cfa with ordinally scaled items
fit_cfa_f1_e <- lavaan::cfa(model_cfa_f1_e,
                         data = dat,
                         ordered = var_f1_e,
                         cluster = "class_id")

summary(fit_cfa_f1_e, fit.measures = TRUE, standardized = TRUE)
# modindices(fit_cfa_f1_e) |> filter(mi > 3.84 & op == "~~") |> arrange(desc(mi))

fit_cfa_f2_e <- lavaan::cfa(model_cfa_f2_e,
                         data = dat,
                         ordered = var_f2_e,
                         cluster = "class_id")

summary(fit_cfa_f2_e, fit.measures = TRUE, standardized = TRUE)
# modindices(fit_cfa_f2_e) |> filter(mi > 3.84 & op == "~~") |> arrange(desc(mi))

fit_cfa_f3_e <- lavaan::cfa(model_cfa_f3_e,
                         data = dat,
                         ordered = var_f3_e,
                         cluster = "class_id")

summary(fit_cfa_f3_e, fit.measures = TRUE, standardized = TRUE)
```

# Internal consistency

I am using the ordinal alphas from Zumbo (2007).

Zumbo, B. D., Gadermann, A. M., & Zeisser, C. (2007). Ordinal versions of coefficients alpha and theta for Likert rating scales. Journal of Modern Applied Statistical Methods, 6(1), 21--29. doi: 10.22237/jmasm/1177992180

## Baseline

Both alpha and omega are acceptable to good.

```{r Baseline internal consistency}
# psych::omegaFromSem(fit_cfa_b)

# reliability(fit_cfa_b)
# alpha.ord 0.7425041 0.7915872 0.7794578

# all: alpha = 0.77, omega_h = 0.82
df <- dat |> select(all_of(var_b))
get.ordinal.omega(df, nfactors = 3) 

# f1: alpha = 0.73, omega = 0.73
df <- dat |> select(all_of(c("capable_person_b", "confident_abt_future_b", "comfortable_who_i_am_b", "feel_smart_b", "respect_lookup_b", "belong_school_b", "people_accept_b", "comfortable_at_school_b")))
get.ordinal.omega(df) 

# f2: alpha = 0.77, omega = 0.77
df <- dat |> select(all_of(c("worried_other_think_b", "worry_abt_dumb_b", "nervous_worried_b", "worry_ppl_dislike_b", "conclusion_other_deaf_b", "conclusion_my_perform_b", "conclusion_abt_me_b")))
get.ordinal.omega(df) 

# f3: alpha = 0.77, omega = 0.77
df <- dat |> select(all_of(c("bad_grades_b", "not_understand_class_b", "not_understand_homework_b", "bad_class_teacher_b", "trouble_studying_b", "pressure_parent_teacher_b")))
get.ordinal.omega(df) 
```

```{r Baseline internal consistency separate}
# https://github.com/simsem/semTools/issues/48
# https://rdrr.io/cran/semTools/man/semTools-deprecated.html
semTools::compRelSEM(fit_cfa_f1_b)
semTools::compRelSEM(fit_cfa_f2_b)
semTools::compRelSEM(fit_cfa_f3_b)
```

## Endline

Both alpha and omega are acceptable. Although they are lower as compared to those at the baseline.

```{r Endline internal consistency}
# psych::omegaFromSem(fit_cfa_e)

# reliability(fit_cfa_e)
# alpha.ord 0.7032811 0.7073264 0.7847403

# all: alpha = 0.75, omega_h = 0.80
df <- dat |> select(all_of(var_e))
get.ordinal.omega(df, nfactors = 3) 

# f1: alpha = 0.64, omega = 0.66
df <- dat |> select(all_of(c("capable_person_e", "confident_abt_future_e", "comfortable_who_i_am_e", "feel_smart_e", "respect_lookup_e", "belong_school_e", "people_accept_e", "comfortable_at_school_e")))
get.ordinal.omega(df) 

# f2: alpha = 0.73, omega = 0.74
df <- dat |> select(all_of(c("worried_other_think_e", "worry_abt_dumb_e", "nervous_worried_e", "worry_ppl_dislike_e", "conclusion_other_deaf_e", "conclusion_my_perform_e", "conclusion_abt_me_e")))
get.ordinal.omega(df) 

# f3: alpha = 0.78, omega = 0.78
df <- dat |> select(all_of(c("bad_grades_e", "not_understand_class_e", "not_understand_homework_e", "bad_class_teacher_e", "trouble_studying_e", "pressure_parent_teacher_e")))
get.ordinal.omega(df) 
```

```{r Baseline internal consistency separate}
# https://github.com/simsem/semTools/issues/48
# https://rdrr.io/cran/semTools/man/semTools-deprecated.html
semTools::compRelSEM(fit_cfa_f1_e)
semTools::compRelSEM(fit_cfa_f2_e)
semTools::compRelSEM(fit_cfa_f3_e)
```

# Create factor scores

```{r}
# baseline
# pred_b <- lavPredict(fit_cfa_b)
# index_b <- inspect(fit_cfa_b, "case.idx")

pred_f1_b <- lavPredict(fit_cfa_f1_b)
index_f1_b <- inspect(fit_cfa_f1_b, "case.idx")

pred_f2_b <- lavPredict(fit_cfa_f2_b)
index_f2_b <- inspect(fit_cfa_f2_b, "case.idx")

pred_f3_b <- lavPredict(fit_cfa_f3_b)
index_f3_b <- inspect(fit_cfa_f3_b, "case.idx")

dat[index_f1_b, "fscore_f1_b"] <- pred_f1_b[,1]
dat[index_f2_b, "fscore_f2_b"] <- pred_f2_b[,1]
dat[index_f3_b, "fscore_f3_b"] <- pred_f3_b[,1]

 
# endline
# pred_e <- lavPredict(fit_cfa_e)
# index_e <- inspect(fit_cfa_e, "case.idx")

pred_f1_e <- lavPredict(fit_cfa_f1_e)
index_f1_e <- inspect(fit_cfa_f1_e, "case.idx")

pred_f2_e <- lavPredict(fit_cfa_f2_e)
index_f2_e <- inspect(fit_cfa_f2_e, "case.idx")

pred_f3_e <- lavPredict(fit_cfa_f3_e)
index_f3_e <- inspect(fit_cfa_f3_e, "case.idx")

dat[index_f1_e, "fscore_f1_e"] <- pred_f1_e[,1]
dat[index_f2_e, "fscore_f2_e"] <- pred_f2_e[,1]
dat[index_f3_e, "fscore_f3_e"] <- pred_f3_e[,1]

dat_fscore <- dat |> select(st_id, fscore_f1_b:fscore_f3_e)
write_csv(dat_fscore, "/Users/michaelfive/Library/CloudStorage/GoogleDrive-wuzezhen33@gmail.com/My Drive/Nepal SA Study/Data/Cleaned/fscore.csv")
```

# Frequentist models

```{r}
dat_s <- dat |> filter(!is.na(treated_int))

# scale within each classroom
dat_s <- dat_s |> 
  group_by(class_id) |> 
  mutate_at(vars(fscore_f1_b:fscore_f3_e), function(x){as.numeric(scale(x))})

dat_s <- dat_s |> select(
  st_id, sch_id, grade, class_id, class_size, gender, age_b, student_type, father_edu_f, mother_edu_f,
    father_occ_salary, mother_occ_salary, adult_members,
  siblings, treated_int, duration_int, mc_survey_1_int:mc_survey_4_int,
  fscore_f1_b:fscore_f3_e)
```

```{r}
dat_mi <- mice::mice(dat_s, m = 5, seed = 1234)

# Extract the imputed datasets
imputed_datasets <- mice::complete(dat_mi, "long", include = TRUE)

# Fit the lmer model on each imputed dataset
fits <- list()

fits <- for (i in 1:5) {
  fits[[i]] <- 
    lmer(fscore_f1_e ~ fscore_f1_b + treated_int +
    class_size + grade + gender + age_b + student_type + father_edu_f + mother_edu_f +
    father_occ_salary + mother_occ_salary + adult_members + siblings + duration_int +
    (treated_int | class_id), 
    data = imputed_datasets[imputed_datasets$.imp == i, ])
}

# Combine the fitted models using the pool function from the lme4 package
pooled_fit <- lme4::pool(fits)
```


# Bayesian models (all students)

```{r}
library(brms)
# library(mice)

dat_mi <- mice::mice(dat_s, m = 5, seed = 1234)
```

```{r}
all_priors <- read_rds("/Users/michaelfive/Library/CloudStorage/GoogleDrive-wuzezhen33@gmail.com/My Drive/Nepal SA Study/Analysis/literature/BASIE/BASIE R and Stan code/all_priors.RDS")

# set prior according to past meta-analysis

fit_f1 <- brm_multiple(
  fscore_f1_e ~ fscore_f1_b + treated_int +
    class_size + grade + gender + age_b + student_type + father_edu_f + mother_edu_f +
    father_occ_salary + mother_occ_salary + adult_members + siblings + duration_int + sch_id +
    (treated_int | class_id),
  dat_mi,
  prior = set_prior("normal(0, 1)", class = "b", coef = "treated_int"), 
  chains = 4,
  seed = 1234
)

fit_f2 <- brm_multiple(
  fscore_f2_e ~ fscore_f2_b + treated_int +
    class_size + grade + gender + age_b + student_type + father_edu_f + mother_edu_f +
    father_occ_salary + mother_occ_salary + adult_members + siblings + duration_int + sch_id +
    (treated_int | class_id),
  dat_mi,
  prior = set_prior("normal(0, 1)", class = "b", coef = "treated_int"), 
  chains = 4,
  seed = 1234
)

fit_f3 <- brm_multiple(
  fscore_f3_e ~ fscore_f3_b + treated_int +
    class_size + grade + gender + age_b + student_type + father_edu_f + mother_edu_f +
    father_occ_salary + mother_occ_salary + adult_members + siblings + duration_int + sch_id +
    (treated_int | class_id),
  dat_mi,
  prior = set_prior("normal(0, 1)", class = "b", coef = "treated_int"), 
  chains = 4,
  seed = 1234
)


```

# Bayesian models (deaf only)

```{r}
dat_deaf <- dat_s |> filter(student_type == "Deaf")
dat_deaf_mi <- mice::mice(dat_deaf, m = 5, seed = 1234)
```

```{r}
fit_f1_deaf <- brm_multiple(
  fscore_f1_e ~ fscore_f1_b + treated_int +
    class_size + grade + gender + age_b + father_edu_f + mother_edu_f +
    father_occ_salary + mother_occ_salary + adult_members + siblings + duration_int +
    (treated_int | class_id),
  dat_deaf_mi,
  prior = set_prior("normal(0, 1)", class = "b", coef = "treated_int"), 
  chains = 4,
  seed = 1234
)

fit_f2_deaf <- brm_multiple(
  fscore_f2_e ~ fscore_f2_b + treated_int +
    class_size + grade + gender + age_b + father_edu_f + mother_edu_f +
    father_occ_salary + mother_occ_salary + adult_members + siblings + duration_int +
    (treated_int | class_id),
  dat_deaf_mi,
  prior = set_prior("normal(0, 1)", class = "b", coef = "treated_int"), 
  chains = 4,
  seed = 1234
)

fit_f3_deaf <- brm_multiple(
  fscore_f3_e ~ fscore_f3_b + treated_int +
    class_size + grade + gender + age_b + father_edu_f + mother_edu_f +
    father_occ_salary + mother_occ_salary + adult_members + siblings + duration_int +
    (treated_int | class_id),
  dat_deaf_mi,
  prior = set_prior("normal(0, 1)", class = "b", coef = "treated_int"), 
  chains = 4,
  seed = 1234
)
```

# Bayes factor threshold
<The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective>

"The choice of decision threshold is set by practical considerations. A Bayes factor between 3 and 10 is supposed to indicate “moderate” or “substantial” evidence for the winning model, while a Bayes factor between 10 and 30 indicates “strong” evidence, and a Bayes factor greater than 30 indicates “very strong” evidence (Jeffreys, 1961; Kass & Raftery, 1995; Wetzels et al., 2011). Dienes (2016) suggested a Bayes factor of 3 for substantial evidence, while Schönbrodt et al. (2016) recommended the decision threshold for a Bayes factor be set at 6 for incipient stages of research but set at a higher threshold of 10 for mature confirmatory research (in the specific context of a null hypothesis test for the means of two groups, implying that the decision threshold might be different for different sorts of analyses). Somewhat analogous to considerations for a ROPE, the decision threshold for a Bayes factor depends on the practical aspects of the application."